{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c942ca80",
   "metadata": {},
   "source": [
    "# Disaster Tweet Analysis: EDA, Cleaning, BERT  \n",
    "Auto‚Äëgenerated notebook skeleton based on your original report  \n",
    "Author: **Mohamed Niyaz**  \n",
    "Created: 2025‚Äë07‚Äë31\n",
    "\n",
    "**How to use:**  \n",
    "1. Make sure `train.csv` and `test.csv` from the Kaggle ‚ÄúReal or Not? Disaster Tweets‚Äù competition are in the same folder as this notebook (or change the paths in `load_data`).  \n",
    "2. Run the notebook top‚Äëto‚Äëbottom.  \n",
    "3. Heavy BERT training is commented out‚Äîuncomment those lines if you have GPU/V100 or Colab Pro.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "220e5d57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\niyaz\\anaconda3\\envs\\nlp-tf\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\niyaz\\anaconda3\\envs\\nlp-tf\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ==============================\n",
    "# 1. Library Imports & Seed\n",
    "# ==============================\n",
    "import gc, re, string, random, operator\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "import os\n",
    "os.environ[\"TRANSFORMERS_NO_KERAS_TENSOR\"] = \"1\"\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import STOPWORDS\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "import tensorflow as tf\n",
    "# import tensorflow_hub as hub\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "\n",
    "SEED = 1337\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "random.seed(SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f488510",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (7613, 5)\n",
      "Test set shape:     (3263, 4)\n"
     ]
    }
   ],
   "source": [
    "# ==============================\n",
    "# 2. Data Loading\n",
    "# ==============================\n",
    "def load_data(train_path='./Data/train.csv', test_path='./Data/test.csv'):\n",
    "    \"\"\"Load the Kaggle disaster tweets dataset.\"\"\"\n",
    "    df_train = pd.read_csv(train_path)\n",
    "    df_test = pd.read_csv(test_path)\n",
    "    print(f'Training set shape: {df_train.shape}')\n",
    "    print(f'Test set shape:     {df_test.shape}')\n",
    "    return df_train, df_test\n",
    "\n",
    "df_train, df_test = load_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17d271c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# 3. Text Cleaning Utilities\n",
    "# ==============================\n",
    "URL_PATTERN = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "HTML_PATTERN = re.compile(r'<.*?>')\n",
    "MENTION_PATTERN = re.compile(r'@[A-Za-z0-9_]+')\n",
    "HASHTAG_PATTERN = re.compile(r'#')\n",
    "PUNCTUATION_TABLE = str.maketrans('', '', string.punctuation)\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    text = URL_PATTERN.sub('', text)\n",
    "    text = HTML_PATTERN.sub('', text)\n",
    "    text = MENTION_PATTERN.sub('', text)\n",
    "    text = HASHTAG_PATTERN.sub('', text)\n",
    "    text = text.translate(PUNCTUATION_TABLE)\n",
    "    return text.lower().strip()\n",
    "\n",
    "for df in (df_train, df_test):\n",
    "    df['text_cleaned'] = df['text'].fillna('').apply(clean_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a558cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# 4. Feature Engineering\n",
    "# ==============================\n",
    "def add_meta_features(df):\n",
    "    df['word_count'] = df['text'].str.split().apply(len)\n",
    "    df['unique_word_count'] = df['text'].str.split().apply(lambda x: len(set(x)))\n",
    "    df['stop_word_count'] = df['text'].str.lower().str.split().apply(lambda x: len([w for w in x if w in STOPWORDS]))\n",
    "    df['url_count'] = df['text'].str.contains('http|https').astype(int)\n",
    "    df['mean_word_length'] = df['text'].str.split().apply(lambda x: np.mean([len(w) for w in x]) if x else 0)\n",
    "    df['char_count'] = df['text'].str.len()\n",
    "    df['punctuation_count'] = df['text'].apply(lambda x: len([c for c in x if c in string.punctuation]))\n",
    "    df['hashtag_count'] = df['text'].str.count('#')\n",
    "    df['mention_count'] = df['text'].str.count('@')\n",
    "    return df\n",
    "\n",
    "df_train = add_meta_features(df_train)\n",
    "df_test  = add_meta_features(df_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d02f94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# 5. BERT Classifier (HF version)\n",
    "# ==============================\n",
    "class BertTweetClassifier:\n",
    "    def __init__(self, max_seq_length=160, lr=1e-5, epochs=2, batch_size=16, folds=3):\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.lr, self.epochs, self.batch_size, self.folds = lr, epochs, batch_size, folds\n",
    "        # HF tokenizer\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    def encode(self, texts):\n",
    "        enc = self.tokenizer(\n",
    "            texts.tolist(),\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_seq_length,\n",
    "            return_tensors='np'\n",
    "        )\n",
    "        # return all three for compatibility\n",
    "        input_ids      = enc['input_ids']\n",
    "        attention_mask = enc['attention_mask']\n",
    "        # token_type_ids is present and is all zeros for single-sentence inputs\n",
    "        token_type_ids = enc.get('token_type_ids', tf.zeros_like(input_ids))\n",
    "        return input_ids, attention_mask, token_type_ids\n",
    "        \n",
    "\n",
    "    def build_model(self):\n",
    "        ids  = Input(shape=(self.max_seq_length,), dtype=tf.int32, name='input_ids')\n",
    "        mask = Input(shape=(self.max_seq_length,), dtype=tf.int32, name='attention_mask')\n",
    "        ttid = Input(shape=(self.max_seq_length,), dtype=tf.int32, name='token_type_ids')\n",
    "\n",
    "        bert_encoder = TFBertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "\n",
    "        # üëâ declare the hidden-state size here (768 for bert-base)\n",
    "        hidden_size = 768\n",
    "\n",
    "        # ‚ú® FIX: pass inputs as a list, not kwargs\n",
    "\n",
    "        cls_token = tf.keras.layers.Lambda(\n",
    "            lambda x: bert_encoder(\n",
    "                input_ids=x[0], attention_mask=x[1], token_type_ids=x[2]\n",
    "                ).last_hidden_state[:, 0, :],\n",
    "                output_shape=(hidden_size,)          # tell Keras the shape\n",
    "                )([ids, mask, ttid])\n",
    "\n",
    "\n",
    "        # ‚îÄ‚îÄ 4. Classification head ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "        x   = tf.keras.layers.Dropout(0.2)(cls_token)\n",
    "        out = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "        model = Model([ids, mask, ttid], out)\n",
    "        model.compile(optimizer=tf.keras.optimizers.Adam(self.lr),\n",
    "                      loss='binary_crossentropy',\n",
    "                      metrics=['accuracy'])\n",
    "        return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2b8f9e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\niyaz\\anaconda3\\envs\\nlp-tf\\lib\\site-packages\\tf_keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "clf = BertTweetClassifier(epochs=1)\n",
    "model = clf.build_model()          # should build without the NotImplementedError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b3704d88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "[[0.4317377 ]\n",
      " [0.62810254]]\n"
     ]
    }
   ],
   "source": [
    "sample_texts = [\"Massive fire downtown\", \"I love sunny days\"]  # 2 dummy tweets\n",
    "ids, mask, ttid = clf.encode(pd.Series(sample_texts))\n",
    "print(model.predict([ids, mask, ttid]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e7b0f16a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded   : (7613, 160) (7613, 160) (7613, 160)\n",
      "Labels    : (7613,) positive = 3271\n"
     ]
    }
   ],
   "source": [
    "# --- Encode tweets -------------------------------------------------\n",
    "ids,  masks,  ttids  = clf.encode(df_train['text_cleaned'])\n",
    "labels               = df_train['target'].values\n",
    "\n",
    "print(\"Encoded   :\", ids.shape, masks.shape, ttids.shape)\n",
    "print(\"Labels    :\", labels.shape, \"positive =\", labels.sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0557866d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m429/429\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m957s\u001b[0m 2s/step - accuracy: 0.4364 - loss: 0.7599 - val_accuracy: 0.4278 - val_loss: 0.7308\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_ids_tr,  X_ids_val, \\\n",
    "X_mask_tr, X_mask_val, \\\n",
    "X_tid_tr,  X_tid_val, \\\n",
    "y_tr,      y_val = train_test_split(\n",
    "    ids, masks, ttids, labels,\n",
    "    test_size=0.10, stratify=labels, random_state=42\n",
    ")\n",
    "\n",
    "model = clf.build_model()\n",
    "\n",
    "history = model.fit(\n",
    "    [X_ids_tr,  X_mask_tr,  X_tid_tr],  y_tr,\n",
    "    validation_data=([X_ids_val, X_mask_val, X_tid_val], y_val),\n",
    "    epochs=1,            # bump to 4-5 later if loss is still dropping\n",
    "    batch_size=16,       # drop to 8 if you hit OOM\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6eddb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75ba851",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
