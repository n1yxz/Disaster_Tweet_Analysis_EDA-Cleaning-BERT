# ğŸŒªï¸ Disaster Tweet Analysis: EDA, Cleaning & BERT

A Natural Language Processing (NLP) project that classifies tweets as **disaster-related** or **not** using **exploratory data analysis**, **text preprocessing**, and a fine-tuned **BERT model**.

---

## ğŸ“‘ Table of Contents

- [ğŸ“Œ Project Description](#-project-description)
- [âœ¨ Key Features](#-key-features)
- [âš™ï¸ Installation](#-installation)
- [ğŸš€ Usage](#-usage)
- [ğŸ“ Project Structure](#-project-structure)
- [ğŸ™Œ Credits](#-credits)
- [ğŸªª License](#-license)
- [ğŸ“¸ Screenshots & Links](#-screenshots--links)

---

## ğŸ“Œ Project Description

This project analyzes tweets related to disasters using a full NLP pipeline â€” starting from **EDA** and **data cleaning** to training a **BERT-based classification model**. The objective is to **identify real disaster tweets** to aid in **faster emergency response** and **automated filtering** of social media content.

---

## âœ¨ Key Features

- ğŸ” Interactive EDA on tweet metadata
- ğŸ§¹ Data cleaning & preprocessing pipeline
- ğŸ¤– BERT-based binary classifier (Hugging Face Transformers)
- ğŸ“Š Evaluation using accuracy, F1-score, confusion matrix
- ğŸ“ˆ Visual insights on data and model performance

---

## âš™ï¸ Installation

1. **Clone the repository**
   ```bash
   git clone https://github.com/n1yxz/Disaster_Tweet_Analysis_EDA-Cleaning-BERT.git
   cd Disaster_Tweet_Analysis_EDA-Cleaning-BERT


1. **Clone the repository:**
   ```bash
   git clone https://github.com/n1yxz/Disaster_Tweet_Analysis_EDA-Cleaning-BERT.git
   cd Disaster_Tweet_Analysis_EDA-Cleaning-BERT
   ```

2. **(Optional) Create and activate a virtual environment:**
   ```bash
   python3 -m venv venv
   source venv/bin/activate  # On Windows: venv\Scripts\activate
   ```

3. **Install requirements:**
   ```bash
   pip install -r requirements.txt
   ```

   Or, if using Jupyter, install dependencies in the notebook cells as needed.

---

## ğŸš€ Usage

- Open the Jupyter notebooks (`.ipynb`) in the repository.
- Follow the notebook cells step by step:
  - Perform EDA to understand the dataset.
  - Apply data cleaning and preprocessing.
  - Train and evaluate the BERT classifier.
- Modify parameters as desired for experimentation.

**Example:**
```bash
jupyter notebook Disaster_Tweet_Analysis_EDA_Cleaning_BERT.ipynb
```

---

## ğŸ“ Project Structure

```
Disaster_Tweet_Analysis_EDA-Cleaning-BERT/
â”œâ”€â”€ data/                # Dataset files (not included in repo)
â”œâ”€â”€ notebooks/           # Main Jupyter notebooks
â”œâ”€â”€ src/                 # Source code and scripts
â”œâ”€â”€ outputs/             # Model outputs, predictions
â”œâ”€â”€ requirements.txt     # Python dependencies
â””â”€â”€ README.md            # Project documentation
```

---

## ğŸ™Œ Credits

- **Contributors:** [n1yxz](https://github.com/n1yxz)
- **Datasets:** [Kaggle Disaster Tweets](https://www.kaggle.com/c/nlp-getting-started/data)
- **Libraries:** HuggingFace Transformers, pandas, numpy, scikit-learn, matplotlib, seaborn

---

## ğŸªª License

This project is licensed under the MIT License. See the [LICENSE](LICENSE) file for details.

---

## ğŸ“¸ Screenshots & Links

![EDA Example](docs/eda_example.png)
![BERT Classification Report](docs/classification_report.png)

- [Project Repo](https://github.com/n1yxz/Disaster_Tweet_Analysis_EDA-Cleaning-BERT)
- [Kaggle Competition](https://www.kaggle.com/c/nlp-getting-started)

---
